from dotenv import load_dotenv
load_dotenv()

import os
import json
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.metrics import FaithfulnessMetric
from deepeval.metrics import HallucinationMetric

# âœ… å¯é€‰ï¼šæˆªæ–­ context é•¿åº¦
def truncate_context(context: str, max_tokens: int = 2000) -> str:
    return context[:max_tokens * 4]

# âœ… è®¾ç½® project æ ¹ç›®å½•è·¯å¾„ï¼ˆtingting_internï¼‰
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

# === âœ… åˆå§‹åŒ–æ‰€æœ‰ metric ===
metrics = {
    "answer_relevancy": AnswerRelevancyMetric(),
    "faithfulness": FaithfulnessMetric(),
    "hallucination": HallucinationMetric()
}

# === âœ… è·¯å¾„å’Œæ¨¡å‹åè®¾ç½® ===
video_id = "justinbieber"
model_key = "justinbieber_mistralai_mistral-small-3.1-24b-instruct_free_analysis.json"

retrieved_file = os.path.join(ROOT_DIR, "extracted_info", f"{video_id}_extract.json")
generated_file = os.path.join(ROOT_DIR, "output", video_id, model_key)
output_path = os.path.join(ROOT_DIR, "eval_results", f"{video_id}_mistral_all_metrics.json")
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# === âœ… åŠ è½½ retrieved context
with open(retrieved_file, "r") as f:
    retrieved_data = json.load(f)
retrieved_context = truncate_context(
    "\n".join([v.get("description", "") for v in retrieved_data.values() if isinstance(v, dict)])
).strip()

# === âœ… åŠ è½½ generated output
with open(generated_file, "r") as f:
    gen_data = json.load(f)
generated_output = "\n".join([f"{k}: {v}" for k, v in gen_data.items() if isinstance(v, str)]).strip()


# === âœ… æ„é€  LLMTestCaseï¼ŒåŒ…æ‹¬ retrieval_context å’Œ context å­—æ®µï¼ˆéƒ½è¦æ˜¯ list[str]ï¼‰
test_case = LLMTestCase(
    input=retrieved_context,
    actual_output=generated_output,
    retrieval_context=[retrieved_context],  # âœ… ä¿®æ­£ä¸º list[str]
    context=[retrieved_context]             # âœ… ä¿®æ­£ä¸º list[str]
)


# === âœ… æ‰§è¡Œæ‰€æœ‰æŒ‡æ ‡è¯„ä¼°
results = {}
for name, metric in metrics.items():
    try:
        score = metric.measure(test_case)
        print(f"ğŸ“Š {name:<20} â†’ Score: {score:.4f}")
        results[name] = float(score)
    except Exception as e:
        print(f"âŒ {name:<20} failed: {e}")
        results[name] = None

# === âœ… ä¿å­˜è¯„ä¼°ç»“æœ
with open(output_path, "w") as f:
    json.dump(results, f, indent=2)

print(f"\nâœ… æ‰€æœ‰æŒ‡æ ‡ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
